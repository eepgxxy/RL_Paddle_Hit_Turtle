{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1 Install required packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -e turtle-0.0.2 -i https://pypi.tuna.tsinghua.edu.cn/simple\n",
    "!pip install parl==1.3.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install tkinter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "paddlepaddle           1.8.0           \n",
      "parl                   1.3.1           \n"
     ]
    }
   ],
   "source": [
    "# 检查依赖包版本是否正确\n",
    "!pip list | grep paddlepaddle\n",
    "!pip list | grep parl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2 Import required packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "from collections import deque\n",
    "import numpy as np\n",
    "\n",
    "import paddle.fluid as fluid\n",
    "import parl\n",
    "from parl import layers\n",
    "from parl.utils import logger\n",
    "import turtle as t\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3 Set parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "LEARN_FREQ = 5 # 训练频率，不需要每一个step都learn，攒一些新增经验后再learn，提高效率\n",
    "MEMORY_SIZE = 500000    # replay memory的大小，越大越占用内存\n",
    "MEMORY_WARMUP_SIZE = 1000  # replay_memory 里需要预存一些经验数据，再从里面sample一个batch的经验让agent去learn\n",
    "BATCH_SIZE = 256   # 每次给agent learn的数据数量，从replay memory随机里sample一批数据出来\n",
    "GAMMA = 0.99 # reward 的衰减因子，一般取 0.9 到 0.999 不等\n",
    "######################################################################\n",
    "######################################################################\n",
    "#\n",
    "# 1. 请设定 learning rate，可以从 0.001 起调，尝试增减\n",
    "#\n",
    "######################################################################\n",
    "######################################################################\n",
    "LEARNING_RATE = 0.0005 # 学习率"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4 Set up Model, Algorithm and Agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### （1）Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Paddle():\n",
    "\n",
    "    def __init__(self):\n",
    "\n",
    "        self.done = False\n",
    "        self.reward = 0\n",
    "        self.hit, self.miss = 0, 0\n",
    "\n",
    "        # Setup Background\n",
    "\n",
    "        self.win = t.Screen()\n",
    "        self.win.title('Paddle')\n",
    "        self.win.bgcolor('black')\n",
    "        self.win.setup(width=600, height=600)\n",
    "        self.win.tracer(0)\n",
    "\n",
    "        # Paddle\n",
    "\n",
    "        self.paddle = t.Turtle()\n",
    "        self.paddle.speed(0)\n",
    "        self.paddle.shape('square')\n",
    "        self.paddle.shapesize(stretch_wid=1, stretch_len=5)\n",
    "        self.paddle.color('white')\n",
    "        self.paddle.penup()\n",
    "        self.paddle.goto(0, -275)\n",
    "\n",
    "        # Ball\n",
    "\n",
    "        self.ball = t.Turtle()\n",
    "        self.ball.speed(0)\n",
    "        self.ball.shape('turtle')\n",
    "        self.ball.color('green')\n",
    "        self.ball.penup()\n",
    "        self.ball.goto(0, 100)\n",
    "        self.ball.dx = 3\n",
    "        self.ball.dy = -3\n",
    "\n",
    "        # Score\n",
    "\n",
    "        self.score = t.Turtle()\n",
    "        self.score.speed(0)\n",
    "        self.score.color('white')\n",
    "        self.score.penup()\n",
    "        self.score.hideturtle()\n",
    "        self.score.goto(0, 250)\n",
    "        self.score.write(\"Hit: {}   Missed: {}\".format(self.hit, self.miss), align='center', font=('Courier', 24, 'normal'))\n",
    "\n",
    "        # -------------------- Keyboard control ----------------------\n",
    "\n",
    "        self.win.listen()\n",
    "        self.win.onkey(self.paddle_right, 'Right')\n",
    "        self.win.onkey(self.paddle_left, 'Left')\n",
    "\n",
    "    # Paddle movement\n",
    "\n",
    "    def paddle_right(self):\n",
    "\n",
    "        x = self.paddle.xcor()\n",
    "        if x < 225:\n",
    "            self.paddle.setx(x+20)\n",
    "\n",
    "    def paddle_left(self):\n",
    "\n",
    "        x = self.paddle.xcor()\n",
    "        if x > -225:\n",
    "            self.paddle.setx(x-20)\n",
    "\n",
    "    # ------------------------ AI control ------------------------\n",
    "\n",
    "    # 0 move left\n",
    "    # 1 do nothing\n",
    "    # 2 move right\n",
    "\n",
    "    def reset(self):\n",
    "\n",
    "        self.paddle.goto(0, -275)\n",
    "        self.ball.goto(0, 100)\n",
    "        return [self.paddle.xcor()*0.01, self.ball.xcor()*0.01, self.ball.ycor()*0.01, self.ball.dx, self.ball.dy]\n",
    "\n",
    "    def step(self, action):\n",
    "\n",
    "        self.reward = 0\n",
    "        self.done = 0\n",
    "\n",
    "        if action == 0:\n",
    "            self.paddle_left()\n",
    "            self.reward -= .1\n",
    "\n",
    "        if action == 2:\n",
    "            self.paddle_right()\n",
    "            self.reward -= .1\n",
    "\n",
    "        self.run_frame()\n",
    "\n",
    "        state = [self.paddle.xcor()*0.01, self.ball.xcor()*0.01, self.ball.ycor()*0.01, self.ball.dx, self.ball.dy]\n",
    "        return self.reward, state, self.done\n",
    "\n",
    "    def run_frame(self):\n",
    "\n",
    "        self.win.update()\n",
    "\n",
    "        # Ball moving\n",
    "\n",
    "        self.ball.setx(self.ball.xcor() + self.ball.dx)\n",
    "        self.ball.sety(self.ball.ycor() + self.ball.dy)\n",
    "\n",
    "        # Ball and Wall collision\n",
    "\n",
    "        if self.ball.xcor() > 290:\n",
    "            self.ball.setx(290)\n",
    "            self.ball.dx *= -1\n",
    "\n",
    "        if self.ball.xcor() < -290:\n",
    "            self.ball.setx(-290)\n",
    "            self.ball.dx *= -1\n",
    "\n",
    "        if self.ball.ycor() > 290:\n",
    "            self.ball.sety(290)\n",
    "            self.ball.dy *= -1\n",
    "\n",
    "        # Ball Ground contact\n",
    "\n",
    "        if self.ball.ycor() < -290:\n",
    "            self.ball.goto(0, 100)\n",
    "            self.miss += 1\n",
    "            self.score.clear()\n",
    "            self.score.write(\"Hit: {}   Missed: {}\".format(self.hit, self.miss), align='center', font=('Courier', 24, 'normal'))\n",
    "            self.reward -= 3\n",
    "            self.done = True\n",
    "\n",
    "        # Ball Paddle collision\n",
    "\n",
    "        if abs(self.ball.ycor() + 250) < 2 and abs(self.paddle.xcor() - self.ball.xcor()) < 55:\n",
    "            self.ball.dy *= -1\n",
    "            self.hit += 1\n",
    "            self.score.clear()\n",
    "            self.score.write(\"Hit: {}   Missed: {}\".format(self.hit, self.miss), align='center', font=('Courier', 24, 'normal'))\n",
    "            self.reward += 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(parl.Model):\n",
    "    def __init__(self, act_dim):\n",
    "        ######################################################################\n",
    "        ######################################################################\n",
    "        #\n",
    "        # 2. 请参考课堂Demo，配置model\n",
    "        #\n",
    "        ######################################################################\n",
    "        ######################################################################\n",
    "        hid1_size = 256\n",
    "        hid2_size = 128\n",
    "        hid3_size = 128\n",
    "        # 3层全连接网络\n",
    "        self.fc1 = layers.fc(size=hid1_size, act='relu')\n",
    "        self.fc2 = layers.fc(size=hid2_size, act='relu')\n",
    "        self.fc3 = layers.fc(size=hid3_size, act='relu')\n",
    "        self.fc4 = layers.fc(size=act_dim, act=None)\n",
    "\n",
    "    def value(self, obs):\n",
    "        # 定义网络\n",
    "        # 输入state，输出所有action对应的Q，[Q(s,a1), Q(s,a2), Q(s,a3)...]\n",
    "        \n",
    "        ######################################################################\n",
    "        ######################################################################\n",
    "        #\n",
    "        # 3. 请参考课堂Demo，组装Q网络\n",
    "        #\n",
    "        ######################################################################\n",
    "        ######################################################################\n",
    "        h1 = self.fc1(obs)\n",
    "        h2 = self.fc2(h1)\n",
    "        h3 = self.fc3(h2)\n",
    "        Q = self.fc4(h3)\n",
    "        return Q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### （2）Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from parl.algorithms import DQN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### （3）Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent(parl.Agent):\n",
    "    def __init__(self,\n",
    "                 algorithm,\n",
    "                 obs_dim,\n",
    "                 act_dim,\n",
    "                 e_greed=0.1,\n",
    "                 e_greed_decrement=0):\n",
    "        assert isinstance(obs_dim, int)\n",
    "        assert isinstance(act_dim, int)\n",
    "        self.obs_dim = obs_dim\n",
    "        self.act_dim = act_dim\n",
    "        super(Agent, self).__init__(algorithm)\n",
    "\n",
    "        self.global_step = 0\n",
    "        self.update_target_steps = 200  # 每隔200个training steps再把model的参数复制到target_model中\n",
    "\n",
    "        self.e_greed = e_greed  # 有一定概率随机选取动作，探索\n",
    "        self.e_greed_decrement = e_greed_decrement  # 随着训练逐步收敛，探索的程度慢慢降低\n",
    "\n",
    "    def build_program(self):\n",
    "        self.pred_program = fluid.Program()\n",
    "        self.learn_program = fluid.Program()\n",
    "\n",
    "        with fluid.program_guard(self.pred_program):  # 搭建计算图用于 预测动作，定义输入输出变量\n",
    "            obs = layers.data(\n",
    "                name='obs', shape=[self.obs_dim], dtype='float32')\n",
    "            self.value = self.alg.predict(obs)\n",
    "\n",
    "        with fluid.program_guard(self.learn_program):  # 搭建计算图用于 更新Q网络，定义输入输出变量\n",
    "            obs = layers.data(\n",
    "                name='obs', shape=[self.obs_dim], dtype='float32')\n",
    "            action = layers.data(name='act', shape=[1], dtype='int32')\n",
    "            reward = layers.data(name='reward', shape=[], dtype='float32')\n",
    "            next_obs = layers.data(\n",
    "                name='next_obs', shape=[self.obs_dim], dtype='float32')\n",
    "            terminal = layers.data(name='terminal', shape=[], dtype='bool')\n",
    "            self.cost = self.alg.learn(obs, action, reward, next_obs, terminal)\n",
    "\n",
    "    def sample(self, obs):\n",
    "        sample = np.random.rand()  # 产生0~1之间的小数\n",
    "        if sample < self.e_greed:\n",
    "            act = np.random.randint(self.act_dim)  # 探索：每个动作都有概率被选择\n",
    "        else:\n",
    "            act = self.predict(obs)  # 选择最优动作\n",
    "        self.e_greed = max(\n",
    "            0.01, self.e_greed - self.e_greed_decrement)  # 随着训练逐步收敛，探索的程度慢慢降低\n",
    "        return act\n",
    "\n",
    "    def predict(self, obs):  # 选择最优动作\n",
    "        obs = np.expand_dims(obs, axis=0)\n",
    "        pred_Q = self.fluid_executor.run(\n",
    "            self.pred_program,\n",
    "            feed={'obs': obs.astype('float32')},\n",
    "            fetch_list=[self.value])[0]\n",
    "        pred_Q = np.squeeze(pred_Q, axis=0)\n",
    "        act = np.argmax(pred_Q)  # 选择Q最大的下标，即对应的动作\n",
    "        return act\n",
    "\n",
    "    def learn(self, obs, act, reward, next_obs, terminal):\n",
    "        # 每隔200个training steps同步一次model和target_model的参数\n",
    "        if self.global_step % self.update_target_steps == 0:\n",
    "            self.alg.sync_target()\n",
    "        self.global_step += 1\n",
    "\n",
    "        act = np.expand_dims(act, -1)\n",
    "        feed = {\n",
    "            'obs': obs.astype('float32'),\n",
    "            'act': act.astype('int32'),\n",
    "            'reward': reward,\n",
    "            'next_obs': next_obs.astype('float32'),\n",
    "            'terminal': terminal\n",
    "        }\n",
    "        cost = self.fluid_executor.run(\n",
    "            self.learn_program, feed=feed, fetch_list=[self.cost])[0]  # 训练一次网络\n",
    "        return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import collections\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class ReplayMemory(object):\n",
    "    def __init__(self, max_size):\n",
    "        self.buffer = collections.deque(maxlen=max_size)\n",
    "\n",
    "    # 增加一条经验到经验池中\n",
    "    def append(self, exp):\n",
    "        self.buffer.append(exp)\n",
    "\n",
    "    # 从经验池中选取N条经验出来\n",
    "    def sample(self, batch_size):\n",
    "        mini_batch = random.sample(self.buffer, batch_size)\n",
    "        obs_batch, action_batch, reward_batch, next_obs_batch, done_batch = [], [], [], [], []\n",
    "\n",
    "        for experience in mini_batch:\n",
    "            s, a, r, s_p, done = experience\n",
    "            obs_batch.append(s)\n",
    "            action_batch.append(a)\n",
    "            reward_batch.append(r)\n",
    "            next_obs_batch.append(s_p)\n",
    "            done_batch.append(done)\n",
    "\n",
    "        return np.array(obs_batch).astype('float32'), \\\n",
    "            np.array(action_batch).astype('float32'), np.array(reward_batch).astype('float32'),\\\n",
    "            np.array(next_obs_batch).astype('float32'), np.array(done_batch).astype('float32')\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.buffer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5 Training && Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 训练一个episode\n",
    "def run_episode(env, agent, rpm):\n",
    "    total_reward = 0\n",
    "    obs = env.reset()\n",
    "    step = 0\n",
    "    while True:\n",
    "        step += 1\n",
    "        action = agent.sample(obs)  # 采样动作，所有动作都有概率被尝试到\n",
    "        reward, next_obs, done = env.step(action)\n",
    "        rpm.append((obs, action, reward, next_obs, done))\n",
    "\n",
    "        # train model\n",
    "        if (len(rpm) > MEMORY_WARMUP_SIZE) and (step % LEARN_FREQ == 0):\n",
    "            (batch_obs, batch_action, batch_reward, batch_next_obs,\n",
    "             batch_done) = rpm.sample(BATCH_SIZE)\n",
    "            train_loss = agent.learn(batch_obs, batch_action, batch_reward,\n",
    "                                     batch_next_obs,\n",
    "                                     batch_done)  # s,a,r,s',done\n",
    "\n",
    "        total_reward += reward\n",
    "        obs = next_obs\n",
    "        if done:\n",
    "            break\n",
    "    return total_reward\n",
    "\n",
    "\n",
    "# 评估 agent, 跑 5 个episode，总reward求平均\n",
    "def evaluate(env, agent, render=False):\n",
    "    eval_reward = []\n",
    "    for i in range(5):\n",
    "        obs = env.reset()\n",
    "        episode_reward = 0\n",
    "        while True:\n",
    "            action = agent.predict(obs)  # 预测动作，只选最优动作\n",
    "            reward, obs, done = env.step(action)\n",
    "            episode_reward += reward\n",
    "            if render:\n",
    "                env.render()\n",
    "            if done:\n",
    "                break\n",
    "        eval_reward.append(episode_reward)\n",
    "    return np.mean(eval_reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_train_rewards=[]\n",
    "all_test_rewards=[]\n",
    "all_train_steps=[]\n",
    "all_test_steps=[]\n",
    "\n",
    "def draw_process(title,episode,reward,label, color):\n",
    "    plt.title(title, fontsize=24)\n",
    "    plt.xlabel(\"episode\", fontsize=20)\n",
    "    plt.ylabel(label, fontsize=20)\n",
    "    plt.plot(episode, reward,color=color,label=label) \n",
    "    plt.legend()\n",
    "    plt.grid()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = Paddle()\n",
    "np.random.seed(0)\n",
    "action_dim = 3  # actions: 3\n",
    "obs_dim = 5  # states: 5\n",
    "\n",
    "# 创建经验池\n",
    "rpm = ReplayMemory(MEMORY_SIZE)  # DQN的经验回放池\n",
    "\n",
    "\n",
    "\n",
    "# 根据parl框架构建agent\n",
    "######################################################################\n",
    "######################################################################\n",
    "#\n",
    "# 嵌套Model, DQN, Agent构建 agent\n",
    "#\n",
    "######################################################################\n",
    "######################################################################\n",
    "model = Model(act_dim=action_dim)\n",
    "algorithm = DQN(model, act_dim=action_dim, gamma=GAMMA, lr=LEARNING_RATE)\n",
    "agent = Agent(\n",
    "    algorithm,\n",
    "    obs_dim=obs_dim,\n",
    "    act_dim=action_dim,\n",
    "    e_greed=0.1,  # 有一定概率随机选取动作，探索\n",
    "    e_greed_decrement=1e-6)  # 随着训练逐步收敛，探索的程度慢慢降低\n",
    "\n",
    "\n",
    "prev_eval_reward = 50\n",
    "# 加载模型\n",
    "# save_path = './dqn_model.ckpt'\n",
    "# agent.restore(save_path)\n",
    "\n",
    "# 先往经验池里存一些数据，避免最开始训练的时候样本丰富度不够\n",
    "while len(rpm) < MEMORY_WARMUP_SIZE:\n",
    "    run_episode(env, agent, rpm)\n",
    "\n",
    "max_episode = 500\n",
    "\n",
    "# 开始训练\n",
    "episode = 0\n",
    "while episode < max_episode:  # 训练max_episode个回合，test部分不计算入episode数量\n",
    "    # train part\n",
    "    for i in range(0, 50):\n",
    "        total_reward = run_episode(env, agent, rpm)\n",
    "        all_train_rewards.append(total_reward)\n",
    "        episode += 1\n",
    "\n",
    "    # test part\n",
    "    eval_reward = evaluate(env, agent, render=False)  # render=True 查看显示效果\n",
    "    all_test_rewards.append(eval_reward)\n",
    "    logger.info('episode:{}    e_greed:{}   test_reward:{}'.format(\n",
    "        episode, agent.e_greed, eval_reward))\n",
    "    if eval_reward > prev_eval_reward:\n",
    "        prev_eval_reward = eval_reward\n",
    "        ckpt = 'episode_{}_reward_{}.ckpt'.format(episode, int(eval_reward))\n",
    "        agent.save('models_dir/'+ckpt)\n",
    "\n",
    "    # if eval_reward > :\n",
    "    #     break\n",
    "# 训练结束，保存模型\n",
    "save_path = 'models_dir/dqn_model.ckpt'\n",
    "agent.save(save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "ckpt = 'model.ckpt' # this model's eval reward is above 463\n",
    "agent.restore(ckpt)\n",
    "evaluate_reward = evaluate(env, agent)\n",
    "logger.info('Evaluate reward: {}'.format(evaluate_reward))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "paddle",
   "language": "python",
   "name": "paddle"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
